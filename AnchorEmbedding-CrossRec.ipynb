{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msV3bnO1fcQi"
      },
      "source": [
        "**# Installation of required librairies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLMfSdxJfkzP"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc3zKNbxflv-",
        "outputId": "7f85f418-b8b2-4224-9c77-f641b269163e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.23.5)\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post10.tar.gz (3.6 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2023.7.22)\n"
          ]
        }
      ],
      "source": [
        "! pip install tqdm\n",
        "! pip install numpy\n",
        "! pip install sklearn\n",
        "! pip install gensim\n",
        "! pip install requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJGoe-TSNbt8"
      },
      "source": [
        "**Download DataSet from github**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpyFP6XgNAl7",
        "outputId": "f267948f-7556-4bb5-b86c-c1959c42f1ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-10-14 20:45:45--  https://raw.githubusercontent.com/msellamiTN/smartcities_repo/main/provider_data/train.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 106732 (104K) [text/plain]\n",
            "Saving to: ‘/content/data_provider/train.txt’\n",
            "\n",
            "\r          /content/   0%[                    ]       0  --.-KB/s               \r/content/data_provi 100%[===================>] 104.23K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-10-14 20:45:45 (3.61 MB/s) - ‘/content/data_provider/train.txt’ saved [106732/106732]\n",
            "\n",
            "--2023-10-14 20:45:45--  https://raw.githubusercontent.com/msellamiTN/smartcities_repo/main/provider_data/test.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 344891 (337K) [text/plain]\n",
            "Saving to: ‘/content/data_provider/test.txt’\n",
            "\n",
            "/content/data_provi 100%[===================>] 336.81K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-10-14 20:45:45 (6.51 MB/s) - ‘/content/data_provider/test.txt’ saved [344891/344891]\n",
            "\n",
            "--2023-10-14 20:45:45--  https://raw.githubusercontent.com/msellamiTN/smartcities_repo/main/provider_data/valid.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 344891 (337K) [text/plain]\n",
            "Saving to: ‘/content/data_provider/valid.txt’\n",
            "\n",
            "/content/data_provi 100%[===================>] 336.81K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2023-10-14 20:45:45 (6.54 MB/s) - ‘/content/data_provider/valid.txt’ saved [344891/344891]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!mkdir data_provider data_provider/dataset1 data_provider/dataset2 data_provider/dataset3\n",
        "!wget -O /content/data_provider/train.txt https://raw.githubusercontent.com/msellamiTN/smartcities_repo/main/provider_data/train.txt\n",
        "!wget -O /content/data_provider/test.txt https://raw.githubusercontent.com/msellamiTN/smartcities_repo/main/provider_data/test.txt\n",
        "!wget -O /content/data_provider/valid.txt https://raw.githubusercontent.com/msellamiTN/smartcities_repo/main/provider_data/valid.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCHM3CJgMjBW"
      },
      "outputs": [],
      "source": [
        "!cp data_provider/test.txt data_provider/dataset1/\n",
        "!cp data_provider/test.txt data_provider/dataset2/\n",
        "!cp data_provider/test.txt data_provider/dataset3/\n",
        "!cp data_provider/valid.txt data_provider/dataset1/\n",
        "!cp data_provider/valid.txt data_provider/dataset2/\n",
        "!cp data_provider/valid.txt data_provider/dataset3/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7bY2pyeMiPO"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWM6G4wGQ7KL",
        "outputId": "4067b3bb-1fa7-4563-c7ce-0c014128242d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-4-a00dad115397>:61: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  subset_users = set(random.sample(users, 300))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "\n",
        "# Read the dataset from the text file into a pandas DataFrame\n",
        "df = pd.read_csv('/content/data_provider/train.txt', sep=' ',names=['Relations','Node1','Node2'])\n",
        "\n",
        "# Dictionary to store the count of each node type\n",
        "node_count = {\n",
        "    'user': 0,\n",
        "    'service': 0\n",
        "}\n",
        "\n",
        "# Dictionary to store the count of each prefix\n",
        "prefix_count = {\n",
        "    'SP1': 0,\n",
        "    'SP2': 0,\n",
        "    'SP3': 0\n",
        "}\n",
        "\n",
        "# Set to store the selected users\n",
        "users = set()\n",
        "\n",
        "# List to store the selected relations\n",
        "dataset = []\n",
        "\n",
        "# Iterate through the relations in the DataFrame\n",
        "for index, row in df.iterrows():\n",
        "    relation = row['Relations']\n",
        "    node1 = row['Node1']\n",
        "    node2 = row['Node2']\n",
        "\n",
        "    # Skip rows with nan values\n",
        "    if pd.isnull(relation) or pd.isnull(node1) or pd.isnull(node2):\n",
        "        continue\n",
        "\n",
        "    # Extract the prefix from the relation\n",
        "    prefix = relation.split('_')[0]\n",
        "\n",
        "    # Increment the count of the prefix\n",
        "    prefix_count[prefix] += 1\n",
        "\n",
        "    # If the relation contains 'U-Similar', add both nodes to the set of users\n",
        "    if 'U-Similar' in relation:\n",
        "        users.add(node1)\n",
        "        users.add(node2)\n",
        "        dataset.append((relation, node1, node2))\n",
        "    # If the relation contains 'Consume' and the node1 is in the set of users, add the relation to the dataset\n",
        "    elif 'Consume' in relation and node1 in users:\n",
        "        dataset.append((relation, node1, node2))\n",
        "    # If the relation contains 'S-Smiliar' and the node1 is in the set of services from the 'Consume' relations, add the relation to the dataset\n",
        "    elif 'S-Smiliar' in relation:\n",
        "        services = set()\n",
        "        for _, row in df[df['Relations'].str.startswith('Consume')].iterrows():\n",
        "            services.add(row['Node2'])\n",
        "        if node1 in services:\n",
        "            dataset.append((relation, node1, node2))\n",
        "\n",
        "# Select a random subset of the users from the set\n",
        "random.shuffle(list(users))\n",
        "subset_users = set(random.sample(users, 300))\n",
        "\n",
        "# Select a subset of the relations that contain a user from the subset of users\n",
        "subset_relations = [r for r in dataset if r[1] in subset_users]\n",
        "\n",
        "# Convert the selected relations to a DataFrame\n",
        "df_subset = pd.DataFrame(subset_relations, columns=['Relations', 'Node1', 'Node2'])\n",
        "\n",
        "# Save the DataFrame to a TSV file\n",
        "\n",
        "df_subset.to_csv('/content/data_provider/dataset1/train.txt', sep=' ', index=False, header=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN7tAXuiBAl_",
        "outputId": "5efc78ff-3156-4026-b697-0b9f2d4e4551"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-5-9532ae924650>:36: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  subset_users = set(random.sample(users, subset_size))\n",
            "<ipython-input-5-9532ae924650>:36: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  subset_users = set(random.sample(users, subset_size))\n",
            "<ipython-input-5-9532ae924650>:36: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  subset_users = set(random.sample(users, subset_size))\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "def generate_subset(df, prefixes, subset_size=300, output_file='dataset.txt'):\n",
        "    # Set to store the selected users\n",
        "    users = set()\n",
        "\n",
        "    # List to store the selected relations\n",
        "    dataset = []\n",
        "\n",
        "    # Iterate through the relations in the DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        relation = row['Relations']\n",
        "        node1 = row['Node1']\n",
        "        node2 = row['Node2']\n",
        "\n",
        "        # Check if the relation prefix is in the desired prefixes\n",
        "        if not any(relation.startswith(prefix) for prefix in prefixes):\n",
        "            continue\n",
        "\n",
        "        # If the relation contains 'U-Similar', add both nodes to the set of users\n",
        "        if 'U-Similar' in relation:\n",
        "            users.add(node1)\n",
        "            users.add(node2)\n",
        "            dataset.append((relation, node1, node2))\n",
        "        # If the relation contains 'Consume' and the node1 is in the set of users, add the relation to the dataset\n",
        "        elif 'Consume' in relation and node1 in users:\n",
        "            dataset.append((relation, node1, node2))\n",
        "        # If the relation contains 'S-Smiliar' and the node1 is in the set of services from the 'Consume' relations, add the relation to the dataset\n",
        "        elif 'S-Smiliar' in relation:\n",
        "            services = {row['Node2'] for _, row in df[df['Relations'].str.startswith('Consume')].iterrows()}\n",
        "            if node1 in services:\n",
        "                dataset.append((relation, node1, node2))\n",
        "\n",
        "    # Select a random subset of the users from the set\n",
        "    subset_users = set(random.sample(users, subset_size))\n",
        "\n",
        "    # Select a subset of the relations that contain a user from the subset of users\n",
        "    subset_relations = [r for r in dataset if r[1] in subset_users]\n",
        "\n",
        "    # Convert the selected relations to a DataFrame\n",
        "    df_subset = pd.DataFrame(subset_relations, columns=['Relations', 'Node1', 'Node2'])\n",
        "\n",
        "    # Save the DataFrame to a file\n",
        "    df_subset.to_csv(output_file, sep=' ', index=False, header=False)\n",
        "\n",
        "\n",
        "# Main code to generate datasets\n",
        "df = pd.read_csv('/content/data_provider/train.txt', sep=' ', names=['Relations', 'Node1', 'Node2'])\n",
        "\n",
        "generate_subset(df, ['SP1'], output_file='/content/data_provider/dataset1/train.txt')\n",
        "generate_subset(df, ['SP1', 'SP2'], output_file='/content/data_provider/dataset2/train.txt')\n",
        "generate_subset(df, ['SP1', 'SP2', 'SP3'], output_file='/content/data_provider/dataset3/train.txt')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juJowBNTqEvB"
      },
      "source": [
        "**Gloabl Parameters of the Cross Network Training Model:**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ujkGhFUJqFHy"
      },
      "outputs": [],
      "source": [
        "file_name = \"/content/data_provider/\"\n",
        "epochs=epoch = 100\n",
        "batch_size = 64\n",
        "eval_type = \"all\"\n",
        "dimensions = 200\n",
        "edge_dim = 10\n",
        "att_dim = 20\n",
        "walk_length = 5\n",
        "num_walks = 10\n",
        "window_size = 5\n",
        "negative_samples = 5\n",
        "neighbor_samples = 10\n",
        "patience = 5\n",
        "aggregator = 'max-pooling'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVtiHs_Xf2HP"
      },
      "source": [
        "**Definition of Differents walks used on the approach**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yQkkPYlf1Jy"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import random\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def preprocess_nxgraph(graph):\n",
        "    node2idx = {}\n",
        "    idx2node = []\n",
        "    node_size = 0\n",
        "    for node in graph.nodes():\n",
        "        node2idx[node] = node_size\n",
        "        idx2node.append(node)\n",
        "        node_size += 1\n",
        "    return idx2node, node2idx\n",
        "\n",
        "def partition_dict(vertices, workers):\n",
        "    batch_size = (len(vertices) - 1) // workers + 1\n",
        "    part_list = []\n",
        "    part = []\n",
        "    count = 0\n",
        "    for v1, nbs in vertices.items():\n",
        "        part.append((v1, nbs))\n",
        "        count += 1\n",
        "        if count % batch_size == 0:\n",
        "            part_list.append(part)\n",
        "            part = []\n",
        "    if len(part) > 0:\n",
        "        part_list.append(part)\n",
        "    return part_list\n",
        "\n",
        "\n",
        "def partition_list(vertices, workers):\n",
        "    batch_size = (len(vertices) - 1) // workers + 1\n",
        "    part_list = []\n",
        "    part = []\n",
        "    count = 0\n",
        "    for v1, nbs in enumerate(vertices):\n",
        "        part.append((v1, nbs))\n",
        "        count += 1\n",
        "        if count % batch_size == 0:\n",
        "            part_list.append(part)\n",
        "            part = []\n",
        "    if len(part) > 0:\n",
        "        part_list.append(part)\n",
        "    return part_list\n",
        "\n",
        "def partition_num(num, workers):\n",
        "    if num % workers == 0:\n",
        "        return [num//workers]*workers\n",
        "    else:\n",
        "        return [num//workers]*workers + [num % workers]\n",
        "\n",
        "class RWGraph():\n",
        "    def __init__(self, nx_G, node_type=None):\n",
        "        self.G = nx_G\n",
        "        self.node_type = node_type\n",
        "    def deepwalk_walk(self, walk_length, start_node):\n",
        "\n",
        "        walk = [start_node]\n",
        "\n",
        "        while len(walk) < walk_length:\n",
        "            cur = walk[-1]\n",
        "            cur_nbrs = list(self.G.neighbors(cur))\n",
        "            if len(cur_nbrs) > 0:\n",
        "                walk.append(random.choice(cur_nbrs))\n",
        "            else:\n",
        "                break\n",
        "        return walk\n",
        "\n",
        "    def node2vec_walk(self, walk_length, start_node):\n",
        "\n",
        "        G = self.G\n",
        "        alias_nodes = self.alias_nodes\n",
        "        alias_edges = self.alias_edges\n",
        "\n",
        "        walk = [start_node]\n",
        "\n",
        "        while len(walk) < walk_length:\n",
        "            cur = walk[-1]\n",
        "            cur_nbrs = list(G.neighbors(cur))\n",
        "            if len(cur_nbrs) > 0:\n",
        "                if len(walk) == 1:\n",
        "                    walk.append(\n",
        "                        cur_nbrs[alias_sample(alias_nodes[cur][0], alias_nodes[cur][1])])\n",
        "                else:\n",
        "                    prev = walk[-2]\n",
        "                    edge = (prev, cur)\n",
        "                    next_node = cur_nbrs[alias_sample(alias_edges[edge][0],\n",
        "                                                      alias_edges[edge][1])]\n",
        "                    walk.append(next_node)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        return walk\n",
        "\n",
        "    def simulate_walks(self, num_walks, walk_length, workers, verbose=0):\n",
        "\n",
        "        G = self.G\n",
        "\n",
        "        nodes = list(G.nodes())\n",
        "\n",
        "        results = Parallel(n_jobs=workers, verbose=verbose, )(\n",
        "            delayed(self._simulate_walks)(nodes, num, walk_length) for num in\n",
        "            partition_num(num_walks, workers))\n",
        "\n",
        "        walks = list(itertools.chain(*results))\n",
        "\n",
        "        return walks\n",
        "\n",
        "    def _simulate_walks(self, nodes, num_walks, walk_length,):\n",
        "        walks = []\n",
        "        for _ in range(num_walks):\n",
        "            random.shuffle(nodes)\n",
        "            for v in nodes:\n",
        "                walks.append(self.deepwalk_walk(walk_length=walk_length, start_node=v))\n",
        "\n",
        "        return walks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNPRVqVXgLX9"
      },
      "source": [
        "**Definition of required methods and tools used on the approach**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c7kPmL0gJz-"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "from collections import defaultdict\n",
        "import joblib\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from gensim.models.keyedvectors import Vocab\n",
        "from six import iteritems\n",
        "from sklearn.metrics import (auc, f1_score, precision_recall_curve,\n",
        "                             roc_auc_score)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def get_G_from_edges(edges):\n",
        "    edge_dict = dict()\n",
        "    for edge in edges:\n",
        "        edge_key = str(edge[0]) + '_' + str(edge[1])\n",
        "        if edge_key not in edge_dict:\n",
        "            edge_dict[edge_key] = 1\n",
        "        else:\n",
        "            edge_dict[edge_key] += 1\n",
        "    tmp_G = nx.Graph()\n",
        "    for edge_key in edge_dict:\n",
        "        weight = edge_dict[edge_key]\n",
        "        x = edge_key.split('_')[0]\n",
        "        y = edge_key.split('_')[1]\n",
        "        tmp_G.add_edge(x, y)\n",
        "        tmp_G[x][y]['weight'] = weight\n",
        "    return tmp_G\n",
        "\n",
        "def load_training_data(f_name):\n",
        "    print('We are loading data from:', f_name)\n",
        "    edge_data_by_type = dict()\n",
        "    all_nodes = list()\n",
        "    with open(f_name, 'r') as f:\n",
        "        for line in f:\n",
        "            words = line[:-1].split(' ')\n",
        "            if words[0] not in edge_data_by_type:\n",
        "                edge_data_by_type[words[0]] = list()\n",
        "            x, y = words[1], words[2]\n",
        "            edge_data_by_type[words[0]].append((x, y))\n",
        "            all_nodes.append(x)\n",
        "            all_nodes.append(y)\n",
        "    all_nodes = list(set(all_nodes))\n",
        "\n",
        "    print('Total training nodes: ' + str(len(all_nodes)))\n",
        "    return edge_data_by_type\n",
        "\n",
        "\n",
        "def load_testing_data(f_name):\n",
        "    print('We are loading data from:', f_name)\n",
        "    true_edge_data_by_type = dict()\n",
        "    false_edge_data_by_type = dict()\n",
        "    all_edges = list()\n",
        "    all_nodes = list()\n",
        "    with open(f_name, 'r') as f:\n",
        "        #i=0\n",
        "        for line in f:\n",
        "            #i=i+1\n",
        "            #print(\"**line:\", line, \"  i=\", i)\n",
        "            words = line[:-1].split(' ')\n",
        "            x, y = words[1], words[2]\n",
        "            if int(words[3]) == 1:\n",
        "                if words[0] not in true_edge_data_by_type:\n",
        "                    true_edge_data_by_type[words[0]] = list()\n",
        "                true_edge_data_by_type[words[0]].append((x, y))\n",
        "            else:\n",
        "                if words[0] not in false_edge_data_by_type:\n",
        "                    false_edge_data_by_type[words[0]] = list()\n",
        "                false_edge_data_by_type[words[0]].append((x, y))\n",
        "            all_nodes.append(x)\n",
        "            all_nodes.append(y)\n",
        "    all_nodes = list(set(all_nodes))\n",
        "    return true_edge_data_by_type, false_edge_data_by_type\n",
        "\n",
        "def generate_walks(network_data, num_walks, walk_length, file_name):\n",
        "    workers = joblib.cpu_count()\n",
        "    print('number of cpu: '+str(workers))\n",
        "    all_walks = []\n",
        "    for layer_id in network_data:\n",
        "        tmp_data = network_data[layer_id]\n",
        "        # start to do the random walk on a layer\n",
        "        layer_walker = RWGraph(get_G_from_edges(tmp_data))\n",
        "        layer_walks = layer_walker.simulate_walks(num_walks, walk_length, workers)\n",
        "\n",
        "        all_walks.append(layer_walks)\n",
        "\n",
        "    print('Finish generating the walks')\n",
        "\n",
        "    return all_walks\n",
        "\n",
        "def generate_pairs(all_walks, vocab, window_size):\n",
        "    pairs = []\n",
        "    skip_window = window_size // 2\n",
        "    for layer_id, walks in enumerate(all_walks):\n",
        "        for walk in walks:\n",
        "            for i in range(len(walk)):\n",
        "                for j in range(1, skip_window + 1):\n",
        "                    if i - j >= 0:\n",
        "                        pairs.append((vocab[walk[i]].index, vocab[walk[i - j]].index, layer_id))\n",
        "                    if i + j < len(walk):\n",
        "                        pairs.append((vocab[walk[i]].index, vocab[walk[i + j]].index, layer_id))\n",
        "    return pairs\n",
        "\n",
        "def generate_vocab(all_walks):\n",
        "    index2word = []\n",
        "    raw_vocab = defaultdict(int)\n",
        "\n",
        "    for walks in all_walks:\n",
        "        for walk in walks:\n",
        "            for word in walk:\n",
        "                raw_vocab[word] += 1\n",
        "\n",
        "    vocab = {}\n",
        "    for word, v in iteritems(raw_vocab):\n",
        "        vocab[word] = Vocab(count=v, index=len(index2word))\n",
        "        index2word.append(word)\n",
        "\n",
        "    index2word.sort(key=lambda word: vocab[word].count, reverse=True)\n",
        "    for i, word in enumerate(index2word):\n",
        "        vocab[word].index = i\n",
        "\n",
        "    return vocab, index2word\n",
        "\n",
        "def get_score(local_model, node1, node2):\n",
        "    try:\n",
        "        vector1 = local_model[node1]\n",
        "        vector2 = local_model[node2]\n",
        "        return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
        "    except Exception as e:\n",
        "        pass\n",
        "\n",
        "\n",
        "def evaluate(model, true_edges, false_edges):\n",
        "    true_list = list()\n",
        "    prediction_list = list()\n",
        "    true_num = 0\n",
        "    for edge in true_edges:\n",
        "        tmp_score = get_score(model, str(edge[0]), str(edge[1]))\n",
        "        if tmp_score is not None:\n",
        "            true_list.append(1)\n",
        "            prediction_list.append(tmp_score)\n",
        "            true_num += 1\n",
        "    for edge in false_edges:\n",
        "        tmp_score = get_score(model, str(edge[0]), str(edge[1]))\n",
        "        if tmp_score is not None:\n",
        "            true_list.append(0)\n",
        "            prediction_list.append(tmp_score)\n",
        "    sorted_pred = prediction_list[:]\n",
        "    sorted_pred.sort()\n",
        "    threshold = sorted_pred[-true_num]\n",
        "    y_pred = np.zeros(len(prediction_list), dtype=np.int32)\n",
        "    for i in range(len(prediction_list)):\n",
        "        if prediction_list[i] >= threshold:\n",
        "            y_pred[i] = 1\n",
        "    y_true = np.array(true_list)\n",
        "    y_scores = np.array(prediction_list)\n",
        "    ps, rs, _ = precision_recall_curve(y_true, y_scores)\n",
        "    return roc_auc_score(y_true, y_scores), f1_score(y_true, y_pred), auc(rs, ps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdIPACCpgynk"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import os\n",
        "import pickle\n",
        "import sys\n",
        "import time\n",
        "from numpy.random import seed\n",
        "import numpy as np\n",
        "# to deal with numpy randomness\n",
        "# seed(1234)\n",
        "import tensorflow.compat.v1 as tf\n",
        "# added to deal with randomness\n",
        "tf.set_random_seed(1234)\n",
        "\n",
        "import tqdm\n",
        "from numpy import random\n",
        "\n",
        "#from utils import *\n",
        "def get_cosine_similarity(embeddings1, embeddings2, node):\n",
        "    try:\n",
        "        vector1 = embeddings1[node]\n",
        "        vector2 = embeddings2[node]\n",
        "        return np.dot(vector1, vector2) / (np.linalg.norm(vector1) * np.linalg.norm(vector2))\n",
        "    except:\n",
        "        return 2+random.random()\n",
        "\n",
        "def get_batches(pairs, neighbors, batch_size):\n",
        "    n_batches = (len(pairs) + (batch_size - 1)) // batch_size\n",
        "\n",
        "    for idx in range(n_batches):\n",
        "        x, y, t, neigh = [], [], [], []\n",
        "        for i in range(batch_size):\n",
        "            index = idx * batch_size + i\n",
        "            if index >= len(pairs):\n",
        "                break\n",
        "            x.append(pairs[index][0])\n",
        "            y.append(pairs[index][1])\n",
        "            t.append(pairs[index][2])\n",
        "            neigh.append(neighbors[pairs[index][0]])\n",
        "        yield (np.array(x).astype(np.int32), np.array(y).reshape(-1, 1).astype(np.int32), np.array(t).astype(np.int32), np.array(neigh).astype(np.int32))\n",
        "\n",
        "def train_model(network_data, log_name, store_file, all_walks):\n",
        "\n",
        "    # Record the start time\n",
        "    start_time = time.time()\n",
        "    vocab, index2word = generate_vocab(all_walks)\n",
        "    train_pairs = generate_pairs(all_walks, vocab, window_size)\n",
        "\n",
        "    edge_types = list(network_data.keys())\n",
        "    print('edge types: '+str(list(network_data.keys())))\n",
        "    num_nodes = len(index2word)\n",
        "    edge_type_count = len(edge_types)\n",
        "    print('edge types '+ str(edge_types))\n",
        "    embedding_size = dimensions # Dimension of the embedding vector.\n",
        "    embedding_u_size =edge_dim\n",
        "    u_num = edge_type_count\n",
        "    num_sampled = negative_samples # Number of negative examples to sample.\n",
        "    dim_a = att_dim\n",
        "    att_head = 1\n",
        "    neighbors = [[[] for __ in range(edge_type_count)] for _ in range(num_nodes)]\n",
        "    for r in range(edge_type_count):\n",
        "        g = network_data[edge_types[r]]\n",
        "        for (x, y) in g:\n",
        "            ix = vocab[x].index\n",
        "            iy = vocab[y].index\n",
        "            neighbors[ix][r].append(iy)\n",
        "            neighbors[iy][r].append(ix)\n",
        "        for i in range(num_nodes):\n",
        "            if len(neighbors[i][r]) == 0:\n",
        "                neighbors[i][r] = [i] * neighbor_samples\n",
        "            elif len(neighbors[i][r]) < neighbor_samples:\n",
        "                neighbors[i][r].extend(list(np.random.choice(neighbors[i][r], size=neighbor_samples-len(neighbors[i][r]))))\n",
        "            elif len(neighbors[i][r]) > neighbor_samples:\n",
        "                neighbors[i][r] = list(np.random.choice(neighbors[i][r], size=neighbor_samples))\n",
        "\n",
        "    graph = tf.Graph()\n",
        "\n",
        "\n",
        "    with graph.as_default():\n",
        "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "\n",
        "\n",
        "        # Parameters to learn\n",
        "        node_embeddings = tf.Variable(tf.random.uniform([num_nodes, embedding_size], -1.0, 1.0))\n",
        "        print(node_embeddings.get_shape().as_list())\n",
        "        node_type_embeddings = tf.Variable(tf.random.uniform([num_nodes, u_num, embedding_u_size], -1.0, 1.0))\n",
        "        print(node_type_embeddings.get_shape().as_list())\n",
        "        trans_weights = tf.Variable(tf.truncated_normal([edge_type_count, embedding_u_size, embedding_size // att_head], stddev=1.0 / math.sqrt(embedding_size)))\n",
        "        trans_weights_s1 = tf.Variable(tf.truncated_normal([edge_type_count, embedding_u_size, dim_a], stddev=1.0 / math.sqrt(embedding_size)))\n",
        "        trans_weights_s2 = tf.Variable(tf.truncated_normal([edge_type_count, dim_a, att_head], stddev=1.0 / math.sqrt(embedding_size)))\n",
        "        nce_weights = tf.Variable(tf.truncated_normal([num_nodes, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
        "        nce_biases = tf.Variable(tf.zeros([num_nodes]))\n",
        "\n",
        "        # Input data\n",
        "        train_inputs = tf.placeholder(tf.int32, shape=[None])\n",
        "        train_labels = tf.placeholder(tf.int32, shape=[None, 1])\n",
        "        train_types = tf.placeholder(tf.int32, shape=[None])\n",
        "        node_neigh = tf.placeholder(tf.int32, shape=[None, edge_type_count, neighbor_samples])\n",
        "\n",
        "        # Look up embeddings for nodes\n",
        "        node_embed = tf.nn.embedding_lookup(node_embeddings, train_inputs)\n",
        "        # to get neighbors embeddings for all nodes in all types of edges: embedding_lookup(params, ids)\n",
        "        node_embed_neighbors = tf.nn.embedding_lookup(node_type_embeddings, node_neigh)\n",
        "\n",
        "        if aggregator == 'max-pooling':\n",
        "            node_embed_tmp = tf.concat([tf.reshape(tf.slice(node_embed_neighbors, [0, i, 0, i, 0], [-1, 1, -1, 1, -1]), [1, -1, neighbor_samples, embedding_u_size]) for i in range(edge_type_count)], axis=0)\n",
        "            node_type_embed = tf.transpose(tf.squeeze(tf.nn.max_pool(node_embed_tmp, ksize=[1, 1, neighbor_samples, 1], strides=[1, 1, neighbor_samples, 1], padding='VALID', data_format='NHWC'), [2]), perm=[1,0,2])\n",
        "            trans_w = tf.nn.embedding_lookup(trans_weights, train_types)\n",
        "            trans_w_s1 = tf.nn.embedding_lookup(trans_weights_s1, train_types)\n",
        "            trans_w_s2 = tf.nn.embedding_lookup(trans_weights_s2, train_types)\n",
        "            attention = tf.reshape(tf.nn.softmax(tf.reshape(tf.matmul(tf.tanh(tf.matmul(node_type_embed, trans_w_s1)), trans_w_s2), [-1, u_num])), [-1, att_head, u_num])\n",
        "            node_type_embed = tf.matmul(attention, node_type_embed)\n",
        "            node_embed = node_embed + tf.reshape(tf.matmul(node_type_embed, trans_w), [-1, embedding_size])\n",
        "\n",
        "        elif aggregator == 'mean':\n",
        "            node_embed_tmp = tf.concat([tf.reshape(tf.slice(node_embed_neighbors, [0, i, 0, i, 0], [-1, 1, -1, 1, -1]), [1, -1, neighbor_samples, embedding_u_size]) for i in range(edge_type_count)], axis=0)\n",
        "            node_type_embed = tf.transpose(tf.reduce_mean(node_embed_tmp, axis=2), perm=[1,0,2])\n",
        "            trans_w = tf.nn.embedding_lookup(trans_weights, train_types)\n",
        "            trans_w_s1 = tf.nn.embedding_lookup(trans_weights_s1, train_types)\n",
        "            trans_w_s2 = tf.nn.embedding_lookup(trans_weights_s2, train_types)\n",
        "            attention = tf.reshape(tf.nn.softmax(tf.reshape(tf.matmul(tf.tanh(tf.matmul(node_type_embed, trans_w_s1)), trans_w_s2), [-1, u_num])), [-1, att_head, u_num])\n",
        "            node_type_embed = tf.matmul(attention, node_type_embed)\n",
        "            node_embed = node_embed + tf.reshape(tf.matmul(node_type_embed, trans_w), [-1, embedding_size])\n",
        "\n",
        "        elif aggregator == 'LSTM':\n",
        "            node_embed_tmp = tf.concat([tf.reshape(tf.slice(node_embed_neighbors, [0, i, 0, i, 0], [-1, 1, -1, 1, -1]), [1, -1, neighbor_samples, embedding_u_size]) for i in range(edge_type_count)], axis=0)\n",
        "            shape = [node_embed_tmp.shape[k] for k in range(4)]\n",
        "            Y=tf.reshape(node_embed_tmp, [-1, shape[2], shape[3]])\n",
        "            lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(embedding_u_size)\n",
        "            node_type_emb, _ = tf.nn.dynamic_rnn(lstm_cell, Y, dtype=tf.float32)\n",
        "            node_type_emb = tf.transpose(node_type_emb, [1, 0, 2])\n",
        "            last = tf.gather(node_type_emb, int(node_type_emb.get_shape()[0]) - 1)\n",
        "            last=tf.reshape(last, [-1, edge_type_count, embedding_u_size])\n",
        "            trans_w = tf.nn.embedding_lookup(trans_weights, train_types)\n",
        "            trans_w_s1 = tf.nn.embedding_lookup(trans_weights_s1, train_types)\n",
        "            trans_w_s2 = tf.nn.embedding_lookup(trans_weights_s2, train_types)\n",
        "\n",
        "            attention = tf.reshape(tf.nn.softmax(tf.reshape(tf.matmul(tf.tanh(tf.matmul(last, trans_w_s1)), trans_w_s2), [-1, u_num])), [-1, att_head, u_num])\n",
        "            last = tf.matmul(attention, last)\n",
        "            node_embed = node_embed + tf.reshape(tf.matmul(last, trans_w), [-1, embedding_size])\n",
        "\n",
        "\n",
        "        last_node_embed = tf.nn.l2_normalize(node_embed, axis=1)\n",
        "\n",
        "        loss = tf.reduce_mean(\n",
        "            tf.nn.nce_loss(\n",
        "                weights=nce_weights,\n",
        "                biases=nce_biases,\n",
        "                labels=train_labels,\n",
        "                inputs=last_node_embed,\n",
        "                num_sampled=num_sampled,\n",
        "                num_classes=num_nodes))\n",
        "        plot_loss = tf.summary.scalar(\"loss\", loss)\n",
        "\n",
        "        # Optimizer.\n",
        "        optimizer = tf.train.AdamOptimizer().minimize(loss, global_step=global_step)\n",
        "\n",
        "        # Add ops to save and restore all the variables.\n",
        "        # saver = tf.train.Saver(max_to_keep=20)\n",
        "\n",
        "        merged = tf.summary.merge_all(key=tf.GraphKeys.SUMMARIES)\n",
        "\n",
        "        # Initializing the variables\n",
        "        init = tf.global_variables_initializer()\n",
        "      # Create an empty DataFrame to store results   average_auc average_f1 average_pr\n",
        "    results_df = pd.DataFrame(columns=['Epoch', 'Iteration', 'Average Loss', 'Loss Value','Average AUC', 'Average F1', 'Loss PR'])\n",
        "\n",
        "\n",
        "    # Launch the graph\n",
        "    print(\"Optimizing\")\n",
        "\n",
        "    with tf.Session(graph=graph) as sess:\n",
        "        writer = tf.summary.FileWriter(\"./runs/\" + log_name, sess.graph) # tensorboard --logdir=./runs\n",
        "        sess.run(init)\n",
        "\n",
        "        print('Training')\n",
        "        g_iter = 0\n",
        "        best_score = 0\n",
        "        patience = 0\n",
        "        for epoch in range(epochs):\n",
        "            random.shuffle(train_pairs)\n",
        "            batches = get_batches(train_pairs, neighbors, batch_size)\n",
        "\n",
        "            data_iter = tqdm.tqdm(batches,\n",
        "                                desc=\"epoch %d\" % (epoch),\n",
        "                                total=(len(train_pairs) + (batch_size - 1)) // batch_size,\n",
        "                                bar_format=\"{l_bar}{r_bar}\")\n",
        "            avg_loss = 0.0\n",
        "\n",
        "            for i, data in enumerate(data_iter):\n",
        "                feed_dict = {train_inputs: data[0], train_labels: data[1], train_types: data[2], node_neigh: data[3]}\n",
        "                _, loss_value, summary_str = sess.run([optimizer, loss, merged], feed_dict)\n",
        "                writer.add_summary(summary_str, g_iter)\n",
        "\n",
        "                g_iter += 1\n",
        "\n",
        "                avg_loss += loss_value\n",
        "\n",
        "                if i % 5000 == 0:\n",
        "                    post_fix = {\n",
        "                        \"epoch\": epoch,\n",
        "                        \"iter\": i,\n",
        "                        \"avg_loss\": avg_loss / (i + 1),\n",
        "                        \"loss\": loss_value\n",
        "                    }\n",
        "                    data_iter.write(str(post_fix))\n",
        "\n",
        "            final_model = dict(zip(edge_types, [dict() for _ in range(edge_type_count)]))\n",
        "            for i in range(edge_type_count):\n",
        "                for j in range(num_nodes):\n",
        "                    final_model[edge_types[i]][index2word[j]] = np.array(sess.run(last_node_embed, {train_inputs: [j], train_types: [i], node_neigh: [neighbors[j]]})[0])\n",
        "\n",
        "            valid_aucs, valid_f1s, valid_prs = [], [], []\n",
        "            test_aucs, test_f1s, test_prs = [], [], []\n",
        "            for i in range(edge_type_count):\n",
        "                if eval_type == 'all' or edge_types[i] in eval_type.split(','):\n",
        "                    tmp_auc, tmp_f1, tmp_pr = evaluate(final_model[edge_types[i]], valid_true_data_by_edge[edge_types[i]], valid_false_data_by_edge[edge_types[i]])\n",
        "                    valid_aucs.append(tmp_auc)\n",
        "                    valid_f1s.append(tmp_f1)\n",
        "                    valid_prs.append(tmp_pr)\n",
        "\n",
        "                    tmp_auc, tmp_f1, tmp_pr = evaluate(final_model[edge_types[i]], testing_true_data_by_edge[edge_types[i]], testing_false_data_by_edge[edge_types[i]])\n",
        "                    test_aucs.append(tmp_auc)\n",
        "                    test_f1s.append(tmp_f1)\n",
        "                    test_prs.append(tmp_pr)\n",
        "            print('valid auc:', np.mean(valid_aucs))\n",
        "            print('valid pr:', np.mean(valid_prs))\n",
        "            print('valid f1:', np.mean(valid_f1s))\n",
        "\n",
        "            average_auc = np.mean(test_aucs)\n",
        "            average_f1 = np.mean(test_f1s)\n",
        "            average_pr = np.mean(test_prs)\n",
        "              # Append the result to the DataFrame\n",
        "            results = {\n",
        "                                  \"epoch\": epoch,\n",
        "                                  \"iter\": i,\n",
        "                                  \"avg_loss\": avg_loss / (i + 1),\n",
        "                                  \"loss\": loss_value,\n",
        "                                  \"average_auc\":average_auc,\n",
        "                                  \"average_f1\" :average_f1,\n",
        "                                  \"average_pr\" :average_f1,\n",
        "                                  \"start_time\":start_time,\n",
        "                                  \"end_time\": time.time()\n",
        "                              }\n",
        "            results_df = results_df.append(results, ignore_index=True)\n",
        "            cur_score = np.mean(valid_aucs)\n",
        "            if cur_score > best_score:\n",
        "                best_score = cur_score\n",
        "                patience = 0\n",
        "            else:\n",
        "                patience += 1\n",
        "                if patience > patience:\n",
        "                    print('Early Stopping')\n",
        "                    break\n",
        "    # Record the end time\n",
        "    end_time = time.time()\n",
        "\n",
        "    # Compute the total training time\n",
        "    training_duration = end_time - start_time\n",
        "    print(f\"Training took {training_duration:.2f} seconds\")\n",
        "    return final_model, average_auc, average_f1, average_pr,results_df,training_duration\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfDiW1STwNxg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KkLTTo9jgo_S"
      },
      "source": [
        "# Cross Network Enbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "-z6u8YE4hYFI",
        "outputId": "3f9941df-601a-4b30-a44b-8b2dabf74f8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We are loading data from: /content/data_provider//train.txt\n",
            "Total training nodes: 1620\n",
            "We are loading data from: /content/data_provider//valid.txt\n",
            "We are loading data from: /content/data_provider//test.txt\n",
            "number of cpu: 2\n",
            "Finish generating the walks\n",
            "edge types: ['SP1_S-Smiliar', 'SP1_Consume', 'SP2_U-Similar', 'SP2_S-Smiliar', 'SP2_Consume', 'SP1_U-Similar', 'SP3_S-Smiliar', 'SP3_U-Similar', 'SP3_Consume']\n",
            "edge types ['SP1_S-Smiliar', 'SP1_Consume', 'SP2_U-Similar', 'SP2_S-Smiliar', 'SP2_Consume', 'SP1_U-Similar', 'SP3_S-Smiliar', 'SP3_U-Similar', 'SP3_Consume']\n",
            "[1620, 200]\n",
            "[1620, 9, 10]\n",
            "Optimizing\n",
            "Training\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 0:   0%|| 5/6644 [00:00<13:05,  8.45it/s]  "
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 0, 'avg_loss': 18.928958892822266, 'loss': 18.928959}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 0:  75%|| 5006/6644 [02:16<00:48, 33.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 0, 'iter': 5000, 'avg_loss': 13.257196434126737, 'loss': 6.43342}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 0: 100%|| 6644/6644 [02:55<00:00, 37.94it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.587953468400653\n",
            "valid pr: 0.632086929467135\n",
            "valid f1: 0.6174821310800745\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 1:   0%|| 5/6644 [00:00<02:36, 42.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 1, 'iter': 0, 'avg_loss': 6.624823570251465, 'loss': 6.6248236}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 1:  75%|| 5005/6644 [01:56<00:49, 33.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 1, 'iter': 5000, 'avg_loss': 7.160967366727823, 'loss': 3.298798}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 1: 100%|| 6644/6644 [02:33<00:00, 43.26it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.3946451058489171\n",
            "valid pr: 0.4904516901297463\n",
            "valid f1: 0.4982971362176161\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 2:   0%|| 5/6644 [00:00<02:31, 43.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 2, 'iter': 0, 'avg_loss': 2.54105281829834, 'loss': 2.5410528}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 2:  75%|| 5005/6644 [01:55<00:53, 30.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 2, 'iter': 5000, 'avg_loss': 4.994568282974837, 'loss': 4.684521}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 2: 100%|| 6644/6644 [02:32<00:00, 43.70it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.4442249087336847\n",
            "valid pr: 0.5153950873019665\n",
            "valid f1: 0.5283547496949619\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 3:   0%|| 5/6644 [00:00<02:22, 46.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 3, 'iter': 0, 'avg_loss': 11.142364501953125, 'loss': 11.1423645}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 3:  75%|| 5004/6644 [01:53<00:50, 32.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 3, 'iter': 5000, 'avg_loss': 3.6943621926845442, 'loss': 1.3240476}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 3: 100%|| 6644/6644 [02:29<00:00, 44.34it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.5344574295306287\n",
            "valid pr: 0.5784824720226011\n",
            "valid f1: 0.5728289749184252\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 4:   0%|| 5/6644 [00:00<02:22, 46.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 4, 'iter': 0, 'avg_loss': 7.496282577514648, 'loss': 7.4962826}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 4:  75%|| 5004/6644 [01:54<00:51, 31.79it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 4, 'iter': 5000, 'avg_loss': 2.8645307462564875, 'loss': 7.6370916}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 4: 100%|| 6644/6644 [02:32<00:00, 43.51it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.628532504887776\n",
            "valid pr: 0.6641940676161896\n",
            "valid f1: 0.6360916876792906\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 5:   0%|| 5/6644 [00:00<02:34, 42.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 5, 'iter': 0, 'avg_loss': 0.9725506901741028, 'loss': 0.9725507}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 5:  75%|| 5003/6644 [01:55<00:51, 31.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 5, 'iter': 5000, 'avg_loss': 2.2426487514553632, 'loss': 2.319981}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 5: 100%|| 6644/6644 [02:32<00:00, 43.62it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6882454227723125\n",
            "valid pr: 0.7321300604410634\n",
            "valid f1: 0.666587943586113\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 6:   0%|| 4/6644 [00:00<02:50, 38.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 6, 'iter': 0, 'avg_loss': 1.1775678396224976, 'loss': 1.1775678}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 6:  75%|| 5006/6644 [01:54<00:50, 32.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 6, 'iter': 5000, 'avg_loss': 1.84978006850169, 'loss': 1.6654782}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 6: 100%|| 6644/6644 [02:31<00:00, 43.77it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.7118126344579122\n",
            "valid pr: 0.7607162653833421\n",
            "valid f1: 0.6861029444375579\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 7:   0%|| 5/6644 [00:00<02:21, 46.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 7, 'iter': 0, 'avg_loss': 1.0696768760681152, 'loss': 1.0696769}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 7:  75%|| 5006/6644 [01:56<00:39, 41.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 7, 'iter': 5000, 'avg_loss': 1.4987276378237613, 'loss': 0.94553757}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 7: 100%|| 6644/6644 [02:33<00:00, 43.32it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.7237298215067467\n",
            "valid pr: 0.7779934314240978\n",
            "valid f1: 0.686655319666684\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 8:   0%|| 4/6644 [00:00<02:55, 37.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 8, 'iter': 0, 'avg_loss': 1.62526273727417, 'loss': 1.6252627}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 8:  75%|| 5005/6644 [01:52<00:51, 31.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 8, 'iter': 5000, 'avg_loss': 1.2924805193859394, 'loss': 1.2066131}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 8: 100%|| 6644/6644 [02:29<00:00, 44.38it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.7161573216715991\n",
            "valid pr: 0.778318952739788\n",
            "valid f1: 0.6826280119578514\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 9:   0%|| 4/6644 [00:00<03:08, 35.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 9, 'iter': 0, 'avg_loss': 1.0089573860168457, 'loss': 1.0089574}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 9:  75%|| 5011/6644 [01:51<00:31, 51.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 9, 'iter': 5000, 'avg_loss': 1.126833101983691, 'loss': 0.7430682}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 9: 100%|| 6644/6644 [02:29<00:00, 44.50it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.726801858289824\n",
            "valid pr: 0.7825811190118178\n",
            "valid f1: 0.6910561247631489\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 10:   0%|| 0/6644 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 10, 'iter': 0, 'avg_loss': 0.6586591005325317, 'loss': 0.6586591}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 10:  75%|| 5008/6644 [01:50<00:34, 47.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 10, 'iter': 5000, 'avg_loss': 0.9798653756075205, 'loss': 1.7156972}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 10: 100%|| 6644/6644 [02:27<00:00, 45.13it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.7182292510572924\n",
            "valid pr: 0.7749272525199671\n",
            "valid f1: 0.6905662713684473\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 11:   0%|| 5/6644 [00:00<02:28, 44.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 11, 'iter': 0, 'avg_loss': 1.1221482753753662, 'loss': 1.1221483}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 11:  75%|| 5008/6644 [01:53<00:34, 48.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 11, 'iter': 5000, 'avg_loss': 0.8753686832221979, 'loss': 0.88737756}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 11: 100%|| 6644/6644 [02:29<00:00, 44.39it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.7182753610313468\n",
            "valid pr: 0.7773027763913807\n",
            "valid f1: 0.6885882814358568\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 12:   0%|| 4/6644 [00:00<02:50, 39.00it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 12, 'iter': 0, 'avg_loss': 1.0766925811767578, 'loss': 1.0766926}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 12:  75%|| 5007/6644 [01:53<00:42, 38.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 12, 'iter': 5000, 'avg_loss': 0.7692886258311044, 'loss': 0.33257648}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 12: 100%|| 6644/6644 [02:29<00:00, 44.43it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.7096835357871014\n",
            "valid pr: 0.7718620087359186\n",
            "valid f1: 0.6849813271349432\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 13:   0%|| 4/6644 [00:00<02:54, 38.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 13, 'iter': 0, 'avg_loss': 0.45478877425193787, 'loss': 0.45478877}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 13:  75%|| 5005/6644 [01:51<00:46, 34.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 13, 'iter': 5000, 'avg_loss': 0.7015648581932317, 'loss': 0.6581329}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 13: 100%|| 6644/6644 [02:29<00:00, 44.57it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.7031463474876454\n",
            "valid pr: 0.7664582498086129\n",
            "valid f1: 0.6813474354945245\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 14:   0%|| 4/6644 [00:00<02:51, 38.78it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 14, 'iter': 0, 'avg_loss': 0.6749542355537415, 'loss': 0.67495424}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 14:  75%|| 5006/6644 [01:51<00:32, 50.06it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 14, 'iter': 5000, 'avg_loss': 0.6365898073440646, 'loss': 0.3159048}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 14: 100%|| 6644/6644 [02:29<00:00, 44.47it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6977017658566136\n",
            "valid pr: 0.7620041156427156\n",
            "valid f1: 0.6692335774936586\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 15:   0%|| 0/6644 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 15, 'iter': 0, 'avg_loss': 0.4181589186191559, 'loss': 0.41815892}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 15:  75%|| 5006/6644 [01:53<00:34, 47.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 15, 'iter': 5000, 'avg_loss': 0.5643326263711634, 'loss': 0.38994628}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 15: 100%|| 6644/6644 [02:30<00:00, 44.13it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6918211257128974\n",
            "valid pr: 0.7558349540334514\n",
            "valid f1: 0.6707558155394716\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 16:   0%|| 4/6644 [00:00<02:58, 37.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 16, 'iter': 0, 'avg_loss': 0.38975751399993896, 'loss': 0.3897575}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 16:  75%|| 5008/6644 [01:55<00:33, 48.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 16, 'iter': 5000, 'avg_loss': 0.5325831443017732, 'loss': 0.43092877}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 16: 100%|| 6644/6644 [02:32<00:00, 43.47it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6891630710566081\n",
            "valid pr: 0.7516008196765732\n",
            "valid f1: 0.6697595225797044\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 17:   0%|| 4/6644 [00:00<02:48, 39.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 17, 'iter': 0, 'avg_loss': 0.3060300350189209, 'loss': 0.30603004}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 17:  75%|| 5010/6644 [01:53<00:34, 47.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 17, 'iter': 5000, 'avg_loss': 0.4944988850089246, 'loss': 0.29337284}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 17: 100%|| 6644/6644 [02:30<00:00, 44.29it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6853998239780894\n",
            "valid pr: 0.7492133946558113\n",
            "valid f1: 0.6669721320192035\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 18:   0%|| 5/6644 [00:00<02:33, 43.23it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 18, 'iter': 0, 'avg_loss': 0.29211121797561646, 'loss': 0.29211122}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 18:  75%|| 5007/6644 [01:53<00:33, 48.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 18, 'iter': 5000, 'avg_loss': 0.4599784149337091, 'loss': 0.495277}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 18: 100%|| 6644/6644 [02:28<00:00, 44.72it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6815404412035708\n",
            "valid pr: 0.7470282424558171\n",
            "valid f1: 0.6599929756013806\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 19:   0%|| 5/6644 [00:00<02:30, 44.25it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 19, 'iter': 0, 'avg_loss': 0.43811875581741333, 'loss': 0.43811876}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 19:  75%|| 5006/6644 [01:51<00:49, 33.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 19, 'iter': 5000, 'avg_loss': 0.4296469114254508, 'loss': 0.32509002}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 19: 100%|| 6644/6644 [02:28<00:00, 44.60it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6735601132313207\n",
            "valid pr: 0.741381547794711\n",
            "valid f1: 0.6597559212334771\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 20:   0%|| 4/6644 [00:00<02:52, 38.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 20, 'iter': 0, 'avg_loss': 0.35667508840560913, 'loss': 0.3566751}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 20:  75%|| 5005/6644 [01:52<00:42, 38.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 20, 'iter': 5000, 'avg_loss': 0.4044440962929913, 'loss': 0.44683146}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 20: 100%|| 6644/6644 [02:29<00:00, 44.32it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6713960000642186\n",
            "valid pr: 0.7394544654848636\n",
            "valid f1: 0.6587210902722559\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 21:   0%|| 0/6644 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 21, 'iter': 0, 'avg_loss': 0.44764506816864014, 'loss': 0.44764507}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 21:  75%|| 5011/6644 [01:51<00:32, 50.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 21, 'iter': 5000, 'avg_loss': 0.37936427123908684, 'loss': 0.3480612}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 21: 100%|| 6644/6644 [02:28<00:00, 44.74it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6706817691382876\n",
            "valid pr: 0.7371073460934876\n",
            "valid f1: 0.6512691661069593\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 22:   0%|| 4/6644 [00:00<02:46, 39.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 22, 'iter': 0, 'avg_loss': 0.3535350263118744, 'loss': 0.35353503}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 22:  75%|| 5007/6644 [01:50<00:33, 48.53it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 22, 'iter': 5000, 'avg_loss': 0.364224261856215, 'loss': 0.18616384}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 22: 100%|| 6644/6644 [02:26<00:00, 45.49it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6609686298862445\n",
            "valid pr: 0.7324611577285105\n",
            "valid f1: 0.6498904505366381\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 23:   0%|| 4/6644 [00:00<02:46, 39.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 23, 'iter': 0, 'avg_loss': 0.22775274515151978, 'loss': 0.22775275}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 23:  75%|| 5005/6644 [01:52<00:49, 33.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 23, 'iter': 5000, 'avg_loss': 0.34872428584941456, 'loss': 0.29901993}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 23: 100%|| 6644/6644 [02:29<00:00, 44.42it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6564872913653803\n",
            "valid pr: 0.727902964141507\n",
            "valid f1: 0.6421045766746639\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 24:   0%|| 4/6644 [00:00<03:05, 35.84it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 24, 'iter': 0, 'avg_loss': 0.3593972325325012, 'loss': 0.35939723}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 24:  75%|| 5007/6644 [01:51<00:35, 46.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 24, 'iter': 5000, 'avg_loss': 0.3363364819803016, 'loss': 0.38082302}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 24: 100%|| 6644/6644 [02:29<00:00, 44.36it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6537120595367388\n",
            "valid pr: 0.7264724763274395\n",
            "valid f1: 0.6455760229882225\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 25:   0%|| 0/6644 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 25, 'iter': 0, 'avg_loss': 0.22613438963890076, 'loss': 0.22613439}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 25:  75%|| 5007/6644 [01:52<00:33, 48.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 25, 'iter': 5000, 'avg_loss': 0.31900573203031407, 'loss': 0.34562674}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 25: 100%|| 6644/6644 [02:29<00:00, 44.32it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6531627531416454\n",
            "valid pr: 0.7285521061724994\n",
            "valid f1: 0.6359372802816604\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 26:   0%|| 5/6644 [00:00<02:35, 42.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 26, 'iter': 0, 'avg_loss': 1.0947842597961426, 'loss': 1.0947843}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 26:  75%|| 5011/6644 [01:52<00:32, 50.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 26, 'iter': 5000, 'avg_loss': 0.3088724251327765, 'loss': 0.31746435}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 26: 100%|| 6644/6644 [02:29<00:00, 44.49it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6468615373032514\n",
            "valid pr: 0.7225718939062831\n",
            "valid f1: 0.6354527419529102\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 27:   0%|| 0/6644 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 27, 'iter': 0, 'avg_loss': 0.28884178400039673, 'loss': 0.28884178}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 27:  75%|| 5008/6644 [01:53<00:33, 48.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 27, 'iter': 5000, 'avg_loss': 0.2954384780141967, 'loss': 0.21938102}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 27: 100%|| 6644/6644 [02:30<00:00, 44.28it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6462251654405806\n",
            "valid pr: 0.7208392148436595\n",
            "valid f1: 0.6391803333725138\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 28:   0%|| 0/6644 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 28, 'iter': 0, 'avg_loss': 0.24240273237228394, 'loss': 0.24240273}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 28:  75%|| 5005/6644 [01:52<00:51, 32.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 28, 'iter': 5000, 'avg_loss': 0.2912507444051022, 'loss': 0.2547912}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 28: 100%|| 6644/6644 [02:30<00:00, 44.15it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6473714048103129\n",
            "valid pr: 0.7187152596621318\n",
            "valid f1: 0.6352435463295274\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 29:   0%|| 4/6644 [00:00<03:02, 36.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 29, 'iter': 0, 'avg_loss': 0.30336540937423706, 'loss': 0.3033654}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 29:  75%|| 5007/6644 [01:54<00:47, 34.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 29, 'iter': 5000, 'avg_loss': 0.2803813687975801, 'loss': 0.28326362}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 29: 100%|| 6644/6644 [02:31<00:00, 43.98it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6414682896332483\n",
            "valid pr: 0.7167146042520461\n",
            "valid f1: 0.634045802765461\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 30:   0%|| 4/6644 [00:00<02:46, 39.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 30, 'iter': 0, 'avg_loss': 0.20981988310813904, 'loss': 0.20981988}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 30:  75%|| 5005/6644 [01:53<00:48, 34.03it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 30, 'iter': 5000, 'avg_loss': 0.2702451594869653, 'loss': 0.32432717}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 30: 100%|| 6644/6644 [02:30<00:00, 44.20it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6353172285032586\n",
            "valid pr: 0.709666837262378\n",
            "valid f1: 0.6299672631378569\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 31:   0%|| 4/6644 [00:00<02:48, 39.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 31, 'iter': 0, 'avg_loss': 0.28985095024108887, 'loss': 0.28985095}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 31:  75%|| 5006/6644 [01:48<00:47, 34.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 31, 'iter': 5000, 'avg_loss': 0.26939493870495607, 'loss': 0.18300828}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 31: 100%|| 6644/6644 [02:24<00:00, 46.01it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6432459681748104\n",
            "valid pr: 0.7153814797901663\n",
            "valid f1: 0.6423331796014264\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 32:   0%|| 5/6644 [00:00<02:34, 42.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 32, 'iter': 0, 'avg_loss': 0.29016348719596863, 'loss': 0.2901635}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 32:  75%|| 5006/6644 [01:46<00:37, 43.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 32, 'iter': 5000, 'avg_loss': 0.2603383668279545, 'loss': 0.38628304}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 32: 100%|| 6644/6644 [02:20<00:00, 47.13it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6405435488765274\n",
            "valid pr: 0.7153580829240506\n",
            "valid f1: 0.6332839087032658\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 33:   0%|| 0/6644 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 33, 'iter': 0, 'avg_loss': 0.13580083847045898, 'loss': 0.13580084}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 33:  75%|| 5011/6644 [01:45<00:31, 51.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 33, 'iter': 5000, 'avg_loss': 0.2560514929747014, 'loss': 0.1884001}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 33: 100%|| 6644/6644 [02:19<00:00, 47.79it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.635295799178729\n",
            "valid pr: 0.7071574129414918\n",
            "valid f1: 0.6315258958883851\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 34:   0%|| 0/6644 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 34, 'iter': 0, 'avg_loss': 0.2518369257450104, 'loss': 0.25183693}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 34:  75%|| 5007/6644 [01:45<00:33, 49.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 34, 'iter': 5000, 'avg_loss': 0.25249439916299526, 'loss': 0.13689034}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 34: 100%|| 6644/6644 [02:18<00:00, 47.83it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6334464844292993\n",
            "valid pr: 0.7106897307117807\n",
            "valid f1: 0.6325810070402164\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 35:   0%|| 4/6644 [00:00<02:59, 36.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 35, 'iter': 0, 'avg_loss': 0.1776636242866516, 'loss': 0.17766362}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 35:  75%|| 5006/6644 [01:45<00:33, 48.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 35, 'iter': 5000, 'avg_loss': 0.24989714420969858, 'loss': 0.41028813}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 35: 100%|| 6644/6644 [02:19<00:00, 47.53it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6309162764955284\n",
            "valid pr: 0.7076805215806359\n",
            "valid f1: 0.6337362468546934\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 36:   0%|| 5/6644 [00:00<02:41, 41.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 36, 'iter': 0, 'avg_loss': 0.18755203485488892, 'loss': 0.18755203}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 36:  75%|| 5005/6644 [01:45<00:31, 51.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 36, 'iter': 5000, 'avg_loss': 0.2411136556483047, 'loss': 0.2439951}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 36: 100%|| 6644/6644 [02:19<00:00, 47.62it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6222275848160577\n",
            "valid pr: 0.7020451705488553\n",
            "valid f1: 0.622722776409069\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 37:   0%|| 5/6644 [00:00<02:31, 43.72it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 37, 'iter': 0, 'avg_loss': 0.23585031926631927, 'loss': 0.23585032}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 37:  75%|| 5010/6644 [01:45<00:31, 51.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 37, 'iter': 5000, 'avg_loss': 0.23917114839383874, 'loss': 0.3771373}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 37: 100%|| 6644/6644 [02:18<00:00, 47.92it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6185336318874115\n",
            "valid pr: 0.697507756482382\n",
            "valid f1: 0.6234272641987171\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 38:   0%|| 5/6644 [00:00<02:33, 43.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 38, 'iter': 0, 'avg_loss': 0.16452044248580933, 'loss': 0.16452044}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 38:  75%|| 5006/6644 [01:45<00:32, 49.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 38, 'iter': 5000, 'avg_loss': 0.2357488929736712, 'loss': 0.48588878}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 38: 100%|| 6644/6644 [02:19<00:00, 47.79it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6186336552789407\n",
            "valid pr: 0.6972575179865712\n",
            "valid f1: 0.6205696752046479\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 39:   0%|| 4/6644 [00:00<02:54, 37.95it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 39, 'iter': 0, 'avg_loss': 0.3121646046638489, 'loss': 0.3121646}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 39:  75%|| 5007/6644 [01:46<00:31, 51.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 39, 'iter': 5000, 'avg_loss': 0.23557893124309762, 'loss': 0.22422428}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 39: 100%|| 6644/6644 [02:20<00:00, 47.38it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6246562530616048\n",
            "valid pr: 0.7010298982011087\n",
            "valid f1: 0.6223431966692395\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 40:   0%|| 4/6644 [00:00<02:47, 39.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 40, 'iter': 0, 'avg_loss': 0.13995417952537537, 'loss': 0.13995418}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 40:  75%|| 5009/6644 [01:47<00:31, 52.02it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 40, 'iter': 5000, 'avg_loss': 0.2359222808815174, 'loss': 0.39539254}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 40: 100%|| 6644/6644 [02:21<00:00, 46.94it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6197472094189157\n",
            "valid pr: 0.697737421990057\n",
            "valid f1: 0.6096163417279752\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 41:   0%|| 4/6644 [00:00<02:56, 37.56it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 41, 'iter': 0, 'avg_loss': 0.12605571746826172, 'loss': 0.12605572}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 41:  75%|| 5010/6644 [01:46<00:33, 48.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 41, 'iter': 5000, 'avg_loss': 0.22994691372168014, 'loss': 0.22962245}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 41: 100%|| 6644/6644 [02:20<00:00, 47.22it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6243366834922095\n",
            "valid pr: 0.6996253861747834\n",
            "valid f1: 0.6199774278456824\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 42:   0%|| 5/6644 [00:00<02:34, 42.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 42, 'iter': 0, 'avg_loss': 0.1718614250421524, 'loss': 0.17186143}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 42:  75%|| 5007/6644 [01:46<00:32, 50.04it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 42, 'iter': 5000, 'avg_loss': 0.22876949846845845, 'loss': 0.18905693}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 42: 100%|| 6644/6644 [02:20<00:00, 47.16it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6217036918138726\n",
            "valid pr: 0.6975497332863099\n",
            "valid f1: 0.6225729132631161\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 43:   0%|| 5/6644 [00:00<02:27, 45.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 43, 'iter': 0, 'avg_loss': 0.18859153985977173, 'loss': 0.18859154}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 43:  75%|| 5008/6644 [01:45<00:32, 49.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 43, 'iter': 5000, 'avg_loss': 0.22778830806438125, 'loss': 0.19097017}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 43: 100%|| 6644/6644 [02:19<00:00, 47.49it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6134157820877307\n",
            "valid pr: 0.6939228950162631\n",
            "valid f1: 0.617024343870536\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 44:   0%|| 0/6644 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 44, 'iter': 0, 'avg_loss': 0.21820038557052612, 'loss': 0.21820039}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 44:  75%|| 5008/6644 [01:44<00:31, 52.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 44, 'iter': 5000, 'avg_loss': 0.22547132439555298, 'loss': 0.23558046}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 44: 100%|| 6644/6644 [02:17<00:00, 48.19it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6099187369098181\n",
            "valid pr: 0.6910894196795924\n",
            "valid f1: 0.6079984204229958\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 45:   0%|| 5/6644 [00:00<02:26, 45.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 45, 'iter': 0, 'avg_loss': 0.13337823748588562, 'loss': 0.13337824}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 45:  75%|| 5008/6644 [01:45<00:32, 50.15it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 45, 'iter': 5000, 'avg_loss': 0.2236150308744749, 'loss': 0.36040863}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 45: 100%|| 6644/6644 [02:18<00:00, 47.89it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6185780120746988\n",
            "valid pr: 0.699232182747295\n",
            "valid f1: 0.611392578296009\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 46:   0%|| 4/6644 [00:00<02:49, 39.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 46, 'iter': 0, 'avg_loss': 0.2457158863544464, 'loss': 0.24571589}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 46:  75%|| 5009/6644 [01:44<00:32, 49.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 46, 'iter': 5000, 'avg_loss': 0.2257298488878317, 'loss': 0.2097572}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 46: 100%|| 6644/6644 [02:17<00:00, 48.24it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6259449385977658\n",
            "valid pr: 0.7042697377175697\n",
            "valid f1: 0.623529229234049\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 47:   0%|| 5/6644 [00:00<02:24, 45.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 47, 'iter': 0, 'avg_loss': 0.24957266449928284, 'loss': 0.24957266}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 47:  75%|| 5009/6644 [01:45<00:31, 51.94it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 47, 'iter': 5000, 'avg_loss': 0.22269514252161102, 'loss': 0.2733912}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 47: 100%|| 6644/6644 [02:18<00:00, 47.83it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6236282262898848\n",
            "valid pr: 0.6999074778872383\n",
            "valid f1: 0.621468753007814\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 48:   0%|| 4/6644 [00:00<02:55, 37.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 48, 'iter': 0, 'avg_loss': 0.29018646478652954, 'loss': 0.29018646}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 48:  75%|| 5010/6644 [01:45<00:32, 51.01it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 48, 'iter': 5000, 'avg_loss': 0.2216257659103472, 'loss': 0.21317518}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 48: 100%|| 6644/6644 [02:18<00:00, 47.95it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6297043625306205\n",
            "valid pr: 0.7036872612402986\n",
            "valid f1: 0.6297753057875588\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 49:   0%|| 5/6644 [00:00<02:30, 44.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 49, 'iter': 0, 'avg_loss': 0.12597133219242096, 'loss': 0.12597133}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 49:  75%|| 5006/6644 [01:45<00:36, 45.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 49, 'iter': 5000, 'avg_loss': 0.22126547258791984, 'loss': 0.15426949}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 49: 100%|| 6644/6644 [02:18<00:00, 47.92it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6315555565399096\n",
            "valid pr: 0.7035793344192302\n",
            "valid f1: 0.630095060604711\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 50:   0%|| 5/6644 [00:00<02:37, 42.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 50, 'iter': 0, 'avg_loss': 0.29869258403778076, 'loss': 0.29869258}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 50:  75%|| 5007/6644 [01:44<00:47, 34.75it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 50, 'iter': 5000, 'avg_loss': 0.21745680010222215, 'loss': 0.18288323}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 50: 100%|| 6644/6644 [02:18<00:00, 47.93it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6149931919981815\n",
            "valid pr: 0.691176470792922\n",
            "valid f1: 0.6155417414417287\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 51:   0%|| 4/6644 [00:00<03:10, 34.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 51, 'iter': 0, 'avg_loss': 0.16742493212223053, 'loss': 0.16742493}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 51:  75%|| 5005/6644 [01:44<00:49, 33.41it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 51, 'iter': 5000, 'avg_loss': 0.2194008816728602, 'loss': 0.1366712}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 51: 100%|| 6644/6644 [02:18<00:00, 47.94it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.618682300897284\n",
            "valid pr: 0.6980321104566019\n",
            "valid f1: 0.6193083994316511\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 52:   0%|| 5/6644 [00:00<02:31, 43.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 52, 'iter': 0, 'avg_loss': 0.19130025804042816, 'loss': 0.19130026}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 52:  75%|| 5008/6644 [01:44<00:45, 36.31it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 52, 'iter': 5000, 'avg_loss': 0.21470603724866003, 'loss': 0.16116309}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 52: 100%|| 6644/6644 [02:18<00:00, 47.84it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.617480391664496\n",
            "valid pr: 0.6955595571433274\n",
            "valid f1: 0.6152801481281897\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 53:   0%|| 0/6644 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 53, 'iter': 0, 'avg_loss': 0.13548165559768677, 'loss': 0.13548166}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 53:  75%|| 5006/6644 [01:44<00:33, 49.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 53, 'iter': 5000, 'avg_loss': 0.21614195403382794, 'loss': 0.12501316}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 53: 100%|| 6644/6644 [02:18<00:00, 47.84it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.6156850680369881\n",
            "valid pr: 0.6906765413334396\n",
            "valid f1: 0.6233771209642265\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 54:   0%|| 5/6644 [00:00<02:40, 41.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 54, 'iter': 0, 'avg_loss': 0.17179226875305176, 'loss': 0.17179227}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 54:  75%|| 5007/6644 [01:43<00:31, 52.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 54, 'iter': 5000, 'avg_loss': 0.2146440476988839, 'loss': 0.35346037}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 54: 100%|| 6644/6644 [02:18<00:00, 47.94it/s]\n",
            "<ipython-input-9-1f5b8b619273>:244: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  results_df = results_df.append(results, ignore_index=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "valid auc: 0.615544991825364\n",
            "valid pr: 0.6918559530725885\n",
            "valid f1: 0.6205778327524337\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 55:   0%|| 0/6644 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 55, 'iter': 0, 'avg_loss': 0.11079271137714386, 'loss': 0.11079271}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 55:  11%|| 715/6644 [00:15<01:53, 52.30it/s]"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "\n",
        "    log_name = file_name.split('/')[-1] + f'_evaltype_{eval_type}_b_{batch_size}_e_{epoch}'\n",
        "\n",
        "    training_data_by_type = load_training_data(file_name + '/train.txt')\n",
        "    valid_true_data_by_edge, valid_false_data_by_edge = load_testing_data(file_name + '/valid.txt')\n",
        "    testing_true_data_by_edge, testing_false_data_by_edge = load_testing_data(file_name + '/test.txt')\n",
        "    all_walks = generate_walks(training_data_by_type, num_walks, walk_length, file_name)\n",
        "    final_model, average_auc, average_f1, average_pr,results_df,training_duration = train_model(training_data_by_type, log_name + '_' + time.strftime('%Y-%m-%d %H-%M-%S',time.localtime(time.time())), 'embeddings1.txt', all_walks)\n",
        "\n",
        "    print('Overall ROC-AUC:', average_auc)\n",
        "    print('Overall PR-AUC', average_pr)\n",
        "    print('Overall F1:', average_f1)\n",
        "    print('Training Duration :', training_duration)\n",
        "     # Save the results DataFrame to a CSV file\n",
        "    results_df.to_csv('results.csv', index=False)\n",
        "    #Use dumps to convert the object to a serialized string\n",
        "    pickle.dump(final_model, open('final_model.emb', 'wb'))\n",
        "    #Use loads to de-serialize an object\n",
        "    #final_model = pickle.load(open('final_model.emb', 'rb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZ631_k1IiNa"
      },
      "outputs": [],
      "source": [
        "print('Overall ROC-AUC:', average_auc)\n",
        "print('Overall PR-AUC', average_pr)\n",
        "print('Overall F1:', average_f1)\n",
        "print('Training Duration :', training_duration)\n",
        "  # Save the results DataFrame to a CSV file\n",
        "results_df.to_csv('results.csv', index=False)\n",
        "#Use dumps to convert the object to a serialized string\n",
        "pickle.dump(final_model, open('final_model.emb', 'wb'))\n",
        "#Use loads to de-serialize an object\n",
        "#final_model = pickle.load(open('final_model.emb', 'rb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRzk9Js9Cb5L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the .tsv file\n",
        "df = pd.read_csv('results.csv', delimiter=',')\n",
        "df.dropna()\n",
        "df=df[['epoch', 'iter', 'avg_loss', 'loss','average_auc', 'average_f1', 'average_pr', 'start_time', 'end_time']]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVgPYp0JCr1g"
      },
      "outputs": [],
      "source": [
        "df.columns = [col.strip() for col in df.columns]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qi4QtPnfxR4w"
      },
      "outputs": [],
      "source": [
        "#['epoch', 'iter', 'avg_loss', 'loss','average_auc', 'average_f1', 'average_pr', 'start_time', 'end_time']\n",
        "# Create LaTeX tikzpicture plot from DataFrame\n",
        "def generate_latex_plot(df, y_column, ylabel):\n",
        "    coordinates = \"\\n\".join(f\"({row['epoch']}, {row[y_column]})\" for _, row in df.iterrows())\n",
        "    return f\"\"\"\n",
        "\\\\begin{{tikzpicture}}\n",
        "\\\\begin{{axis}}[\n",
        "    title={{{ylabel} vs. Epoch}},\n",
        "    xlabel={{Epoch}},\n",
        "    ylabel={{{ylabel}}},\n",
        "    grid=major,\n",
        "    legend entries={{{ylabel}}},\n",
        "]\n",
        "\\\\addplot coordinates {{\n",
        "{coordinates}\n",
        "}};\n",
        "\\\\end{{axis}}\n",
        "\\\\end{{tikzpicture}}\n",
        "\"\"\"\n",
        "\n",
        "# Generating the plots\n",
        "auc_plot = generate_latex_plot(df, 'average_auc', 'average_auc')\n",
        "pr_plot = generate_latex_plot(df, 'average_pr', 'average_pr')\n",
        "f1_plot = generate_latex_plot(df, 'average_f1', 'average_f1')\n",
        "\n",
        "latex_script = f\"\"\"\n",
        "\\\\documentclass{{standalone}}\n",
        "\\\\usepackage{{pgfplots}}\n",
        "\\\\pgfplotsset{{compat=1.15}}\n",
        "\n",
        "\\\\begin{{document}}\n",
        "{auc_plot}\n",
        "{pr_plot}\n",
        "{f1_plot}\n",
        "\\\\end{{document}}\n",
        "\"\"\"\n",
        "\n",
        "# Save to a .tex file\n",
        "with open('plots.tex', 'w') as file:\n",
        "    file.write(latex_script)\n",
        "\n",
        "print(\"LaTeX script saved to plots.tex\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C3ELw-iFsGge"
      },
      "outputs": [],
      "source": [
        "# save the model to disk\n",
        "filename = 'final_model.emb'\n",
        "pickle.dump(final_model, open(filename, 'wb'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6OZdET4suh5"
      },
      "outputs": [],
      "source": [
        "final_model.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaMPsFLKsx7t"
      },
      "outputs": [],
      "source": [
        "#(final_model.values())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r76jAeEpCX_t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + math.exp(-x))\n",
        "\n",
        "def LocateAnchor(active_user_id, network):\n",
        "    \"\"\"Locate the embedding of the active user in the given network.\"\"\"\n",
        "    return network.get(active_user_id, None)\n",
        "\n",
        "def Neighbors(anchor_embedding, user_network):\n",
        "    \"\"\"Retrieve embeddings of users who are similar to the anchor.\"\"\"\n",
        "    similar_users_embeddings = [emb for user, emb in user_network.items() if sigmoid(np.dot(anchor_embedding, emb)) > 0.5]\n",
        "    return similar_users_embeddings\n",
        "\n",
        "def services(user_embedding, service_network):\n",
        "    \"\"\"Return services consumed by users who have similar embeddings.\"\"\"\n",
        "    return [service for service, service_emb in service_network.items() if sigmoid(np.dot(user_embedding, service_emb)) > 0.5]\n",
        "\n",
        "def Rating(user_embedding, service_embedding, alpha=0.5):\n",
        "    if user_embedding is None or service_embedding is None:\n",
        "        return 0\n",
        "    return alpha * sigmoid(np.dot(user_embedding, service_embedding))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v47gHF6sDXwe"
      },
      "outputs": [],
      "source": [
        "def cross_network_service_recommendation(embeddings, active_user_id, top_k=5):\n",
        "    S_top_k = {}\n",
        "\n",
        "    for network_type, network in embeddings.items():\n",
        "        # Work with user similarity networks\n",
        "        if 'U-Similar' in network_type:\n",
        "            anchor_embedding = LocateAnchor(active_user_id, network)\n",
        "\n",
        "            if anchor_embedding is not None:\n",
        "                similar_users_embeddings = Neighbors(anchor_embedding, network)\n",
        "\n",
        "                service_network_type = network_type.replace('U-Similar', 'Consume')\n",
        "                service_network = embeddings[service_network_type]\n",
        "\n",
        "                for user_embedding in similar_users_embeddings:\n",
        "                    user_services = services(user_embedding, service_network)\n",
        "\n",
        "                    for service in user_services:\n",
        "                        if service not in S_top_k:\n",
        "                            S_top_k[service] = Rating(anchor_embedding, service_network[service])\n",
        "\n",
        "    # Sort services based on ratings and return top-k services\n",
        "    sorted_services = sorted(S_top_k, key=S_top_k.get, reverse=True)\n",
        "\n",
        "    return sorted_services[:top_k]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSv5cCViLXTW"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open(\"/content/final_model.emb\", \"rb\") as file:\n",
        "    final_model = pickle.load(file)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhkqULcaDcLp"
      },
      "outputs": [],
      "source": [
        "# Suppose final_model is your embeddings dictionary\n",
        "top_services = cross_network_service_recommendation(final_model, '470500', top_k=5)\n",
        "print(top_services)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL043jo9DlG6"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def LocateAnchor(active_user, user_network):\n",
        "    return user_network.get(active_user, None)\n",
        "\n",
        "def Neighbors(anchor_embedding, user_similarity_network):\n",
        "    similar_users = [user for user, embedding in user_similarity_network.items()\n",
        "                     if sigmoid(np.dot(anchor_embedding, embedding)) > 0.5]\n",
        "    return similar_users\n",
        "\n",
        "def Services(user, service_network):\n",
        "    # Assuming the service network stores services consumed by each user in a list\n",
        "\n",
        "    return service_network.get(user, [])\n",
        "\n",
        "def Rating(user_embedding, service_embedding, alpha=0.5):\n",
        "    \"\"\"Compute the rating for a user-service pair.\"\"\"\n",
        "    # Given the context, we're treating the same embedding as both v and x (or y for services).\n",
        "    latent_factor_product = np.dot(user_embedding, service_embedding)\n",
        "    sigmoid_product = sigmoid(np.dot(user_embedding, service_embedding))\n",
        "    return latent_factor_product + alpha * sigmoid_product\n",
        "\n",
        "def cross_network_service_recommendation(embeddings, active_user, top_k=10,dataset_nb=4):\n",
        "    ratings = {}\n",
        "\n",
        "    # Error checks\n",
        "    if not isinstance(embeddings, dict):\n",
        "        raise ValueError(\"The embeddings parameter should be a dictionary.\")\n",
        "    for i in range(1, dataset_nb):\n",
        "        if f'SP{i}_U-Similar' not in embeddings or f'SP{i}_Consume' not in embeddings:\n",
        "            raise KeyError(f\"Embeddings dictionary is missing keys for SP{i}. Ensure it has both SP{i}_U-Similar and SP{i}_Consume.\")\n",
        "\n",
        "    # Main recommendation logic\n",
        "    for i in range(1, dataset_nb):\n",
        "        user_similarity_network = embeddings[f'SP{i}_U-Similar']\n",
        "        service_network = embeddings[f'SP{i}_Consume']\n",
        "\n",
        "        active_user_embedding = LocateAnchor(active_user, user_similarity_network)\n",
        "        if active_user_embedding is None:\n",
        "            continue\n",
        "\n",
        "        similar_users = Neighbors(active_user_embedding, user_similarity_network)\n",
        "        for user in similar_users:\n",
        "            consumed_services = Services(user, service_network)\n",
        "            #print(consumed_services)\n",
        "            for service, service_embedding in service_network.items():\n",
        "\n",
        "                if service not in consumed_services: # Avoid recommending already recommanded services\n",
        "                    ratings[service] = Rating(active_user_embedding, service_embedding)\n",
        "\n",
        "    # Sort services by their scores to get top recommendations\n",
        "    sorted_ratings = sorted(ratings.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_ratings[:top_k]\n",
        "\n",
        "# Example usage:\n",
        "# final_model = Your embeddings dictionary\n",
        "top_k_services = cross_network_service_recommendation(final_model, '470500', top_k=4,dataset_nb=3)\n",
        "\n",
        "print(top_k_services)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QT8fSmvvITDi"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def get_average_score(top_k_services):\n",
        "    \"\"\"\n",
        "    Compute the average rating from the top k services.\n",
        "    \"\"\"\n",
        "    total_score = sum([score for _, score in top_k_services])\n",
        "    return total_score / len(top_k_services)\n",
        "\n",
        "# Varying k values\n",
        "k_values = list(range(2, 11, 2))\n",
        "\n",
        "# Storing average scores\n",
        "average_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    top_k_services = cross_network_service_recommendation(final_model, '470500', top_k=k,dataset_nb=3)\n",
        "    avg_score = get_average_score(top_k_services)\n",
        "    average_scores.append(avg_score)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(k_values, average_scores, marker='o', linestyle='-', color='b')\n",
        "plt.title(\"Average Rating Score vs. k (#Number of services)\")\n",
        "plt.xlabel(\"k (#Number of services)\")\n",
        "plt.ylabel(\"Average Rating Score\")\n",
        "plt.grid(True)\n",
        "plt.xticks(k_values)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LhbPRIgMhUG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def get_average_score(top_k_services):\n",
        "    \"\"\"\n",
        "    Compute the average rating from the top k services.\n",
        "    \"\"\"\n",
        "    total_score = sum([score for _, score in top_k_services])\n",
        "    return total_score / len(top_k_services)\n",
        "\n",
        "# Varying k values\n",
        "k_values = list(range(2, 11, 2))\n",
        "\n",
        "# Storing average scores\n",
        "average_scores = []\n",
        "\n",
        "for k in k_values:\n",
        "    top_k_services = cross_network_service_recommendation(final_model, '470500', top_k=k)\n",
        "    avg_score = get_average_score(top_k_services)\n",
        "    average_scores.append(avg_score)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame({\n",
        "    'k': k_values,\n",
        "    'Average Rating Score': average_scores\n",
        "})\n",
        "\n",
        "# Create LaTeX tikzpicture plot from DataFrame\n",
        "def generate_latex_plot(df, y_column, ylabel):\n",
        "    coordinates = \"\\n\".join(f\"({row['k']}, {row[y_column]})\" for _, row in df.iterrows())\n",
        "    return f\"\"\"\n",
        "\\\\begin{{tikzpicture}}\n",
        "\\\\begin{{axis}}[\n",
        "    title={{{ylabel} vs. k (#Number of services)}},\n",
        "    xlabel={{k (#Number of services)}},\n",
        "    ylabel={{{ylabel}}},\n",
        "    grid=major,\n",
        "    legend entries={{{ylabel}}},\n",
        "]\n",
        "\\\\addplot coordinates {{\n",
        "{coordinates}\n",
        "}};\n",
        "\\\\end{{axis}}\n",
        "\\\\end{{tikzpicture}}\n",
        "\"\"\"\n",
        "\n",
        "# Generating the plot\n",
        "average_score_plot = generate_latex_plot(df, 'Average Rating Score', 'Average Rating Score')\n",
        "\n",
        "latex_script = f\"\"\"\n",
        "\\\\documentclass{{standalone}}\n",
        "\\\\usepackage{{pgfplots}}\n",
        "\\\\pgfplotsset{{compat=1.15}}\n",
        "\n",
        "\\\\begin{{document}}\n",
        "{average_score_plot}\n",
        "\\\\end{{document}}\n",
        "\"\"\"\n",
        "\n",
        "# Save to a .tex file\n",
        "with open('average_score_plot.tex', 'w') as file:\n",
        "    file.write(latex_script)\n",
        "\n",
        "print(\"LaTeX script saved to average_score_plot.tex\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}